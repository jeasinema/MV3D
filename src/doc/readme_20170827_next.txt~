So how about the fix?
0. enrich training set  OK
1. fix the lookup region, extend it to ...   OK
2. add extra penalty for behaviors like giving a positive sample a low score, both RPN and 2nd stage  OK
	-> to see if we can handle the problem in 264, so as to provide more proposals for target objects
	-> the penalty should be made w.r.t. NMS's threshold. 
3. seperately trained a RPN and a feature extractor first for more interation, then use them to train a fuse net
4. explicitly control the sample amount for 2nd stage during training stage  OK
	-> to see if giving more proposal and set the severe penalty, the bv and rgb can reduce the missing TP(true positive) amount
5. redesign the fusion structure, to explicitly make the rgd, front, lidar supply for each other instead of compete -> you can make a balance for compete and supply using some novel mechanism
(in fact, refer to the result of MV3D, it seems that MV3D == BFV3D, rgb takes a little part. so improve your FV is very important. 
 -> STILL NEED SOME FIX ON LOSS

6. for introduce context aware information for BV, since it is extremely sparse, how can we do it?
-> Maybe the only thing I can do now is to enlarge the bbox   OK
-> Maybe you should take a look at the feature embedding of BV 

7. design some structure to make the proposal turnable, since the 2nd cannot turn the angle of proposal well
-> In fact, I think this maybe partly due to the anchors design, you can just draw and cal, then you will find out that for 45* tilt object, there is no anchor can be treated as positive samples, which means that the RPN do not learn how to regress to such object.
-> after some exp, I found this idea is not correct, since although we shoud improve the anchor design, for such object, RPN can still generate enough proposals. And during the exp, I've also found that the the with_front_8 version do not have the problem of generating proposal with high belief in the side. -> this is may because that for the fusion_img_front, I set the cls loss weight for negative sample to 0.05, which is small and cannot limit it.
so add more weight for negative one or increase the training iterations.
-> A naive method may be first train a BV detector, then stack some height regress layer on it, but such method is not end2end.


8. increase the training iterations.
9. to design some structure for height regression
10. pay attention to the sample amount, loss weight, pos/neg ratio when you enlarge the size of the RPN input. 
11. since the lidar input is very sparse, you cannot require the RPN to give you accurate objectness result of each anchors, so we must lower the threshold of NMS in RPN stage  see 12, OK
12. just remove the RPN NMS during training, since if you do not train the RPN and the RCNN simultaneously, or so-called you train RPN first, there maybe noting can be regarded as negative sample in 2nd stage detector or just non-sense sample like the anchors in the road side  OK
13. add decov layer for the top view before proposal generation OK
14. change the way you encode the front, now it seems that only 3.74% point is remained.....(most of the point occluded..)  SUSPEND
. For the RPN/RCNN param(configuration.py):  OK
[+] RPN_BATCHSIZE: because we want to supress the fp in RPN, increase this can make the fp easier to be generate loss for training RPN
[0] RPN_FG/BG_THRESH
[+] RPN_NMS_PRE/POST_TOPN: this is origin reason why we got very few proposal in 2nd stage during the training precedure
[0] RCNN_BATCH_SIZE: this is just because 128 is what cxz mentioned in his paper
[+/-] RCNN_BG_THRESH: because we use it now in the fuse_target
[0] RCNN_FG_THRESH: have no idea how to fine-tune it, so just remains it unchanged
16. add extra weight of the point amount in a bbox, to prevent the model from overriding the far/occluded vehicles
(in fact, this is a somehow hard sample mining)
